# ğŸ—ï¸ Architecture Overview

Understanding the StockPulse Data Pipeline system design and data flow.

## ğŸ¯ System Overview

StockPulse is a **modern ETL pipeline** designed for financial data processing with real-time visualization capabilities.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Yahoo Finance â”‚â”€â”€â”€â–¶â”‚  ETL Pipeline    â”‚â”€â”€â”€â–¶â”‚   Supabase DB   â”‚
â”‚      API        â”‚    â”‚  (Transform)     â”‚    â”‚  (PostgreSQL)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                        â”‚
         â”‚                       â–¼                        â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Data Analysis   â”‚    â”‚   Streamlit     â”‚
                        â”‚   (Jupyter)      â”‚    â”‚   Dashboard     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow Architecture

### 1. **Data Ingestion Layer**
- **Source**: Yahoo Finance API via `yfinance` library
- **Frequency**: On-demand (manual trigger)
- **Data**: OHLCV (Open, High, Low, Close, Volume) data
- **Timeframe**: 60 days historical data

### 2. **Processing Layer**
- **Transformation**: Data cleaning and feature engineering
- **Validation**: Data quality checks and error handling
- **Enrichment**: Technical indicators calculation

### 3. **Storage Layer**
- **Database**: Supabase (PostgreSQL)
- **Connection**: Pooler for scalability
- **Schema**: Optimized for time-series data

### 4. **Presentation Layer**
- **Dashboard**: Streamlit web application
- **Analytics**: Jupyter notebooks
- **Visualization**: Altair and matplotlib charts

## ğŸ“¦ Component Architecture

### Core Components

```
stockpulse-data-pipeline/
â”œâ”€â”€ ğŸ“Š Data Layer
â”‚   â”œâ”€â”€ Raw Data (CSV)
â”‚   â”œâ”€â”€ Processed Data (CSV)
â”‚   â””â”€â”€ Database Tables
â”‚
â”œâ”€â”€ ğŸ”§ Processing Layer
â”‚   â”œâ”€â”€ Ingestion (fetch_stocks.py)
â”‚   â”œâ”€â”€ Transformation (transform.py)
â”‚   â”œâ”€â”€ Loading (load_to_db.py)
â”‚   â””â”€â”€ Orchestration (pipeline.py)
â”‚
â”œâ”€â”€ ğŸ“ˆ Presentation Layer
â”‚   â”œâ”€â”€ Web Dashboard (app.py)
â”‚   â””â”€â”€ Analytics (analysis.ipynb)
â”‚
â””â”€â”€ âš™ï¸ Configuration Layer
    â”œâ”€â”€ Database Config (db_config.py)
    â””â”€â”€ Environment (.env)
```

## ğŸ—„ï¸ Database Architecture

### Table Structure

Each stock has its own table with standardized schema:

```sql
-- Example: stock_reliance_ns
CREATE TABLE stock_reliance_ns (
    date                DATE PRIMARY KEY,
    close              DECIMAL(10, 2) NOT NULL,
    high               DECIMAL(10, 2) NOT NULL,
    low                DECIMAL(10, 2) NOT NULL,
    open               DECIMAL(10, 2) NOT NULL,
    volume             BIGINT NOT NULL,
    daily_return       DECIMAL(8, 6),
    sma_20            DECIMAL(10, 2),
    created_at         TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Indexing Strategy

```sql
-- Primary index on date for time-series queries
CREATE INDEX idx_stock_date ON stock_reliance_ns(date);

-- Composite index for range queries
CREATE INDEX idx_stock_date_close ON stock_reliance_ns(date, close);

-- Index for analytical queries
CREATE INDEX idx_stock_volume ON stock_reliance_ns(volume) 
WHERE volume > 1000000;
```

## ğŸ”„ ETL Pipeline Design

### Extract Phase
```python
# Data Ingestion Flow
Yahoo Finance API â†’ yfinance â†’ Raw CSV Files
â”‚
â”œâ”€â”€ Error Handling: API rate limits, network issues
â”œâ”€â”€ Data Validation: Schema verification, null checks
â””â”€â”€ File Storage: Timestamped raw data files
```

### Transform Phase
```python
# Data Processing Flow
Raw CSV â†’ pandas DataFrame â†’ Feature Engineering â†’ Validation
â”‚
â”œâ”€â”€ Data Cleaning: Remove duplicates, handle missing values
â”œâ”€â”€ Feature Engineering: Daily returns, moving averages
â”œâ”€â”€ Quality Checks: Data consistency, outlier detection
â””â”€â”€ Format Standardization: Date parsing, decimal precision
```

### Load Phase
```python
# Database Loading Flow
Processed DataFrame â†’ SQLAlchemy â†’ Supabase PostgreSQL
â”‚
â”œâ”€â”€ Connection Management: Pooling, retry logic
â”œâ”€â”€ Table Operations: CREATE, INSERT, UPDATE
â”œâ”€â”€ Transaction Safety: Rollback on errors
â””â”€â”€ Performance: Batch inserts, indexing
```

## ğŸŒ Web Application Architecture

### Streamlit Framework
```python
# Dashboard Architecture
Streamlit App â†’ Database Connection â†’ Data Loading â†’ Visualization
â”‚
â”œâ”€â”€ Caching: @st.cache_data for performance
â”œâ”€â”€ State Management: Session state for user interactions  
â”œâ”€â”€ Reactive Updates: Auto-refresh on data changes
â””â”€â”€ Responsive Design: Mobile-friendly layouts
```

### Frontend Components
- **Navigation**: Sidebar with stock/metric selection
- **Data Display**: Interactive tables and charts
- **Analysis**: Real-time calculations and indicators
- **Export**: Data download capabilities

## ğŸ”§ Configuration Management

### Environment Configuration
```python
# Configuration Hierarchy
1. Environment Variables (.env file)
2. Default Values (fallbacks)
3. Runtime Parameters (command line)

# Security Layers
â”œâ”€â”€ Credential Isolation: .env not in version control
â”œâ”€â”€ Connection Encryption: SSL/TLS for database
â””â”€â”€ Access Control: Supabase RLS policies
```

### Database Connection
```python
# Connection Pool Architecture
Application â†’ SQLAlchemy Engine â†’ Connection Pool â†’ Supabase
â”‚
â”œâ”€â”€ Pool Size: Configurable connections (default: 5)
â”œâ”€â”€ Timeout: Connection and query timeouts
â”œâ”€â”€ Retry Logic: Automatic reconnection on failures
â””â”€â”€ Health Checks: Regular connection validation
```

## ğŸ“Š Data Processing Architecture

### Real-time Processing
```python
# Processing Pipeline
Raw Data â†’ Validation â†’ Transformation â†’ Enrichment â†’ Storage
â”‚
â”œâ”€â”€ Parallel Processing: Multi-stock concurrent processing
â”œâ”€â”€ Memory Management: Chunked processing for large datasets
â”œâ”€â”€ Error Recovery: Graceful handling of failures
â””â”€â”€ Monitoring: Logging and progress tracking
```

### Technical Indicators Engine
```python
# Indicator Calculation Flow
Price Data â†’ Rolling Windows â†’ Mathematical Operations â†’ Signals
â”‚
â”œâ”€â”€ SMA: Simple Moving Average calculations
â”œâ”€â”€ Returns: Daily percentage change calculations
â”œâ”€â”€ Volatility: Rolling standard deviation
â””â”€â”€ Signals: Buy/sell/hold recommendations
```

## ğŸš€ Scalability Considerations

### Horizontal Scaling
- **Multiple Stocks**: Easy addition of new tickers
- **Data Sources**: Pluggable data provider architecture
- **Geographic Distribution**: Multi-region deployment capability

### Vertical Scaling
- **Database**: Supabase auto-scaling
- **Compute**: Configurable processing resources
- **Storage**: Unlimited data retention

### Performance Optimization
- **Caching**: Multi-level caching strategy
- **Indexing**: Optimized database indexes
- **Compression**: Efficient data storage formats
- **CDN**: Static asset delivery optimization

## ğŸ” Security Architecture

### Data Security
```python
# Security Layers
1. Transport Layer: HTTPS/TLS encryption
2. Authentication: Supabase authentication system
3. Authorization: Row-level security policies
4. Data Protection: Encrypted at rest
```

### Application Security
- **Environment Isolation**: Separate dev/prod environments
- **Credential Management**: Secure secret storage
- **Input Validation**: SQL injection prevention
- **Access Logging**: Audit trail for all operations

## ğŸ“ˆ Monitoring and Observability

### Application Monitoring
- **Error Tracking**: Exception logging and alerting
- **Performance Metrics**: Response time monitoring
- **Usage Analytics**: Dashboard interaction tracking
- **Health Checks**: System availability monitoring

### Data Quality Monitoring
- **Data Freshness**: Last update timestamps
- **Data Completeness**: Missing value detection
- **Data Accuracy**: Outlier and anomaly detection
- **Data Consistency**: Cross-validation checks

## ğŸ”® Future Architecture Enhancements

### Planned Improvements
1. **Real-time Streaming**: Apache Kafka for live data
2. **Machine Learning**: Predictive models integration
3. **API Layer**: REST API for external integrations
4. **Microservices**: Service decomposition for scalability
5. **Container Deployment**: Docker and Kubernetes
6. **Data Lake**: Raw data archival and analytics

### Technology Roadmap
- **Event-Driven Architecture**: Message queues and events
- **Serverless Functions**: Cloud functions for processing
- **Graph Database**: Relationship analysis capabilities
- **Time Series DB**: Specialized TSDB for performance

---

## ğŸ“š Related Documentation

- **[Database Schema](database.md)** - Detailed table structures
- **[Configuration Guide](configuration.md)** - Setup and tuning
- **[Development Guide](development.md)** - Contributing guidelines
- **[Performance Guide](performance.md)** - Optimization tips

---

*This architecture supports the current needs while providing a foundation for future enhancements and scale.*